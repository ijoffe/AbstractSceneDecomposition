{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Project &mdash; Isaac Joffe and Benjamin Colussi\n",
    "\n",
    "### CS 679: Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This auxiliary Jupyter notebook breaks down our \"NoRMask\" extension of [MONet](https://arxiv.org/abs/1901.11390), presented by Burgess et al. in \"MONet: Unsupervised Scene Decomposition and Representation\", in detail.\n",
    "\n",
    "This notebook goes through each part of our code for the NoRMask version of the model, explaining the changes from the original model. For a thorough explanation of the original model, see the `monet.ipynb` file contained within this repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cpu\n"
     ]
    }
   ],
   "source": [
    "# Isaac Joffe and Benjamin Colussi, 2025\n",
    "\n",
    "\n",
    "# Fundamental PyTorch utilities to build model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "# Libraries to use datasets\n",
    "from multi_object_datasets_torch import ClevrWithMasks, MultiDSprites, ObjectsRoom, Tetrominoes\n",
    "from multi_object_datasets_torch import flatten_and_one_hot, adjusted_rand_index\n",
    "from arc_data import ARCAGI\n",
    "# Additional common libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "\n",
    "# Get GPU information\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device being used: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "### Summary\n",
    "\n",
    "The original MONet model tasks the VAE with reconstructing each mask generated by the attention network alongside the masked component itself. However, these reconstructed masks tend to be quite poor and are not actually used anywhere in the application of the model; when inferencing the model and reconstructing images, the actual generated masks are used. The only place where this information is used is in the loss function.\n",
    "\n",
    "The alterations made to each component of the network are explained in greater detail below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component: VAE\n",
    "\n",
    "#### Changes\n",
    "\n",
    "To remove the requirement to reconstruct each attention mask in the VAE, the output dimensions generated by the VAE were changed from 4 to 3 channels. Now, the output of the decoder represents only the means of the reconstruction of the masked image component (in RGB format) and the channel representing the mask probabilities was eliminated. Because of the flexible and extensible nature of our code for the MONet model, making this adjustment was easy. No other changes were required for the VAE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VAE of the system.\n",
    "\"\"\"\n",
    "class VAE_NoRMask(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates the VAE, building the encoder and decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Pass along batch size to ensure dimensions are consistent\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Encoder of the VAE\n",
    "        # \"It receives the concatenation of the input image x ...\n",
    "        #     and the attention mask in logarithmic units, log mk as input.\"\n",
    "        self.encoder_nn = nn.Sequential(\n",
    "            # \"The VAE encoder is a standard CNN with 3x3 kernels, stride 2, and ReLU activations.\"\n",
    "            # \"The CNN layers output (32, 32, 64, 64) channels respectively.\"\n",
    "            # Layer #1\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=4,      # \"the concatenation of the input image x and the attention mask\"\n",
    "                out_channels=32,    # \"32\"\n",
    "                kernel_size=3,      # \"3x3 kernels\"\n",
    "                stride=2,           # \"stride 2\"\n",
    "                padding=1,          # ASSUMPTION\n",
    "                bias=True,          # ASSUMPTION\n",
    "            ),                      # 32*32 output because I=64, K=3, S=2, P=1, floor((64-3+2(1))/2)+1=32\n",
    "            nn.ReLU(),              # \"ReLU activations\"\n",
    "            # Layer #2\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=32,     # FROM PREVIOUS LAYER\n",
    "                out_channels=32,    # \"32\"\n",
    "                kernel_size=3,      # \"3x3 kernels\"\n",
    "                stride=2,           # \"stride 2\"\n",
    "                padding=1,          # ASSUMPTION\n",
    "                bias=True,          # ASSUMPTION\n",
    "            ),                      # 16*16 output because I=32, K=3, S=2, P=1, floor((32-3+2(1))/2)+1=16\n",
    "            nn.ReLU(),              # \"ReLU activations\"\n",
    "            # Layer #3\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=32,     # FROM PREVIOUS LAYER\n",
    "                out_channels=64,    # \"64\"\n",
    "                kernel_size=3,      # \"3x3 kernels\"\n",
    "                stride=2,           # \"stride 2\"\n",
    "                padding=1,          # ASSUMPTION\n",
    "                bias=True,          # ASSUMPTION\n",
    "            ),                      # 8*8 output because I=16, K=3, S=2, P=1, floor((16-3+2(1))/2)+1=8\n",
    "            nn.ReLU(),              # \"ReLU activations\"\n",
    "            # Layer #4\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=64,     # FROM PREVIOUS LAYER\n",
    "                out_channels=64,    # \"64\"\n",
    "                kernel_size=3,      # \"3x3 kernels\"\n",
    "                stride=2,           # \"stride 2\"\n",
    "                padding=1,          # ASSUMPTION\n",
    "                bias=True,          # ASSUMPTION\n",
    "            ),                      # 4*4 output because I=8, K=3, S=2, P=1, floor((8-3+2(1))/2)+1=4\n",
    "            nn.ReLU(),              # \"ReLU activations\"\n",
    "            # \"The CNN output is flattened and fed to a 2 layer MLP with output sizes of (256, 32).\"\n",
    "            # \"The MLP output parameterises the μ and log σ of a 16-dim Gaussian latent posterior.\"\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(              # \"MLP\"\n",
    "                in_features=1024,   # FROM PREVIOUS LAYER\n",
    "                out_features=256,   # \"256\"\n",
    "            ),                      # ASSUMPTION (no activation)\n",
    "            nn.Linear(              # \"MLP\"\n",
    "                in_features=256,    # FROM PREVIOUS LAYER\n",
    "                out_features=32,    # \"32\"\n",
    "            ),                      # ASSUMPTION (no activation)\n",
    "        )\n",
    "\n",
    "        # Decoder of the VAE\n",
    "        self.decoder_nn = nn.Sequential(\n",
    "            # \"The input to the broadcast decoder is a spatial tiling of zk concatenated with ...\n",
    "            #     a pair of coordinate channels – one for each spatial dimension – ranging from -1 to 1.\"\n",
    "            # \"These go through a four-layer CNN with no padding, 3x3 kernels, ...\n",
    "            #     stride 1, 32 output channels and ReLU activations.\"\n",
    "            # Layer #1\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=18,     # \"a spatial tiling of zk concatenated with a pair of coordinate channels\"\n",
    "                out_channels=32,    # \"32 output channels\"\n",
    "                kernel_size=3,      # \"3x3 kernels\"\n",
    "                stride=1,           # \"stride 1\"\n",
    "                padding=0,          # \"no padding\"\n",
    "                bias=True,          # ASSUMPTION\n",
    "            ),                      # 70*70 output because I=72, K=3, S=1, P=0, floor((72-3+2(0))/1)+1=70\n",
    "            nn.ReLU(),              # \"ReLU activations\"\n",
    "            # Layer #2\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=32,     # FROM PREVIOUS LAYER\n",
    "                out_channels=32,    # \"32 output channels\"\n",
    "                kernel_size=3,      # \"3x3 kernels\"\n",
    "                stride=1,           # \"stride 1\"\n",
    "                padding=0,          # \"no padding\"\n",
    "                bias=True,          # ASSUMPTION\n",
    "            ),                      # 68*68 output because I=70, K=3, S=1, P=0, floor((70-3+2(0))/1)+1=68\n",
    "            nn.ReLU(),              # \"ReLU activations\"\n",
    "            # Layer #3\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=32,     # FROM PREVIOUS LAYER\n",
    "                out_channels=32,    # \"32 output channels\"\n",
    "                kernel_size=3,      # \"3x3 kernels\"\n",
    "                stride=1,           # \"stride 1\"\n",
    "                padding=0,          # \"no padding\"\n",
    "                bias=True,          # ASSUMPTION\n",
    "            ),                      # 66*66 output because I=68, K=3, S=1, P=0, floor((68-3+2(0))/1)+1=66\n",
    "            nn.ReLU(),              # \"ReLU activations\"\n",
    "            # Layer #4\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=32,     # FROM PREVIOUS LAYER\n",
    "                out_channels=32,    # \"32 output channels\"\n",
    "                kernel_size=3,      # \"3x3 kernels\"\n",
    "                stride=1,           # \"stride 1\"\n",
    "                padding=0,          # \"no padding\"\n",
    "                bias=True,          # ASSUMPTION\n",
    "            ),                      # 64*64 output because I=66, K=3, S=1, P=0, floor((66-3+2(0))/1)+1=64\n",
    "            nn.ReLU(),              # \"ReLU activations\"\n",
    "            # Remove reconstructed mask from output (CHANGE)\n",
    "            # \"A final 1x1 convolutional layer transforms the output into 4 channels: ...\n",
    "            #     3 RGB channels for the means of the image components xˆk, and ...\n",
    "            #     1 for the logits used for the softmax operation to compute the reconstructed attention masks mˆk.\"\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=32,     # FROM PREVIOUS LAYER\n",
    "                out_channels=3,     # CHANGE\n",
    "                kernel_size=1,      # \"1x1 convolutional layer\"\n",
    "                stride=1,           # ASSUMPTION\n",
    "                padding=0,          # ASSUMPTION\n",
    "                bias=True,          # ASSUMPTION\n",
    "            ),                      # 64*64 output because I=64, K=1, S=1, P=0, floor((64-1+2(0))/1)+1=64\n",
    "        )\n",
    "        return\n",
    "        \n",
    "    \"\"\"\n",
    "    Uses the encoder of the VAE to generate the latent distribution.\n",
    "        Inputs: 64*64 RGB image (x), 64*64 logarithmic mask (log_mk)\n",
    "        Outputs: 16-dimensional Gaussian latent posterior (mu, log_sig)\n",
    "    \"\"\"\n",
    "    def encode(self, x, log_mk):\n",
    "        # Encode the input into a latent representation\n",
    "        # \"It receives the concatenation of the input image x and ...\n",
    "        #     the attention mask in logarithmic units, log mk as input.\"\n",
    "        latent_repr = self.encoder_nn(torch.concat((x, log_mk), dim=1))\n",
    "        # assert (len(latent_repr.shape) == 2) and (latent_repr.shape[0] == self.batch_size) and (latent_repr.shape[1] == 32)\n",
    "\n",
    "        # Convert this latent representation into the probability distribution\n",
    "        # \"The MLP output parameterises the μ and logσ of a 16-dim Gaussian latent posterior.\"\n",
    "        mu = torch.split(latent_repr, 16, dim=1)[0]\n",
    "        log_sig = torch.split(latent_repr, 16, dim=1)[1]\n",
    "        # assert (len(mu.shape) == 2) and (mu.shape[0] == self.batch_size) and (mu.shape[1] == 16)\n",
    "        # assert (len(log_sig.shape) == 2) and (log_sig.shape[0] == self.batch_size) and (log_sig.shape[1] == 16)\n",
    "\n",
    "        # Output of the encoder is the parameters of the probability distribution\n",
    "        return mu, log_sig\n",
    "    \n",
    "    \"\"\"\n",
    "    Samples the latent distribution to generate a latent representation.\n",
    "        Inputs: 16-dimensional Gaussian latent posterior (mu, log_sig)\n",
    "        Outputs: Sampled latent vector (z)\n",
    "    \"\"\"\n",
    "    def reparameterize(self, mu, log_sig):\n",
    "        # Sample the represented distribution based on its mean and standard deviation\n",
    "        std = torch.exp(log_sig)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        # assert (len(z.shape) == 2) and (z.shape[0] == self.batch_size) and (z.shape[1] == 16)\n",
    "\n",
    "        # Output of reparameterization is sampled latent vector\n",
    "        return z\n",
    "    \n",
    "    \"\"\"\n",
    "    Uses the decoder of the VAE to reconstruct a component of the image and the mask.\n",
    "        Inputs: 72*72*18 broadcasted sampled representation\n",
    "        Outputs: 64*64 RGB reconstructed image component means (reconstructed_repr)\n",
    "    \"\"\"\n",
    "    def decode(self, x):\n",
    "        # Decode the output from a latent representation\n",
    "        # Convert this output into the reconstructed image but no mask (CHANGE)\n",
    "        # \"3 RGB channels for the means of the image components xˆk, and 1 for the logits ...\n",
    "        #     used for the softmax operation to compute the reconstructed attention masks mˆk\"\n",
    "        reconstructed_repr = self.decoder_nn(x)\n",
    "        # assert (len(reconstructed_repr.shape) == 4) and (reconstructed_repr.shape[0] == self.batch_size) and (reconstructed_repr.shape[1] == 3) and (reconstructed_repr.shape[2] == 64) and (reconstructed_repr.shape[3] == 64)\n",
    "\n",
    "        # Output of the decoder is the reconstructed image component means but no mask (CHANGE)\n",
    "        return reconstructed_repr\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs a full forward pass of the VAE, including both encoding and decoding.\n",
    "        Inputs: 64*64 RGB image (x), 64*64 logarithmic mask (log_mk)\n",
    "        Outputs: 16-dimensional Gaussian latent posterior (mu, log_sig), 64*64 RGB reconstructed image ...\n",
    "            component means (x_hat_means)\n",
    "    \"\"\"\n",
    "    def forward(self, x, log_mk):\n",
    "        # First, encode the data into the latent space\n",
    "        mu, log_sig = self.encode(x, log_mk)\n",
    "        # assert (len(mu.shape) == 2) and (mu.shape[0] == self.batch_size) and (mu.shape[1] == 16)\n",
    "        # assert (len(log_sig.shape) == 2) and (log_sig.shape[0] == self.batch_size) and (log_sig.shape[1] == 16)\n",
    "\n",
    "        # Second, transform the latent distributions into a sampled image\n",
    "        # \"The input to the broadcast decoder is a spatial tiling of zk ...\n",
    "        #     concatenated with a pair of coordinate channels - one for ...\n",
    "        #     each spatial dimension - ranging from -1 to 1.\"\n",
    "        z = self.reparameterize(mu, log_sig)\n",
    "        # assert (len(z.shape) == 2) and (z.shape[0] == self.batch_size) and (z.shape[1] == 16)\n",
    "        z = z.reshape((self.batch_size, 16, 1, 1)).repeat((1, 1, 72, 72))\n",
    "        # assert (len(z.shape) == 4) and (z.shape[0] == self.batch_size) and (z.shape[1] == 16) and (z.shape[2] == 72) and (z.shape[3] == 72)\n",
    "        dim1 = torch.linspace(-1, 1, 72, device=device)\n",
    "        dim2 = torch.linspace(-1, 1, 72, device=device)\n",
    "        dim1, dim2 = torch.meshgrid(dim1, dim2, indexing=\"ij\")\n",
    "        dim1 = dim1.reshape((1, 1, 72, 72)).repeat((self.batch_size, 1, 1, 1))\n",
    "        dim2 = dim2.reshape((1, 1, 72, 72)).repeat((self.batch_size, 1, 1, 1))\n",
    "        # assert (len(dim1.shape) == 4) and (dim1.shape[0] == self.batch_size) and (dim1.shape[1] == 1) and (dim1.shape[2] == 72) and (dim1.shape[3] == 72)\n",
    "        # assert (len(dim2.shape) == 4) and (dim2.shape[0] == self.batch_size) and (dim2.shape[1] == 1) and (dim2.shape[2] == 72) and (dim2.shape[3] == 72)\n",
    "\n",
    "        # Third, decode the data from the latent space\n",
    "        x_hat_means = self.decode(torch.concat((z, dim1, dim2), dim=1))\n",
    "        # assert (len(x_hat_means.shape) == 4) and (x_hat_means.shape[0] == self.batch_size) and (x_hat_means.shape[1] == 3) and (x_hat_means.shape[2] == 64) and (x_hat_means.shape[3] == 64)\n",
    "\n",
    "        # Output of the overall VAE is the parameters of the probability distribution and the reconstructed image and mask\n",
    "        return mu, log_sig, x_hat_means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component: Attention\n",
    "\n",
    "#### Changes\n",
    "\n",
    "No changes were required for the attention network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Attention network of the system.\n",
    "\"\"\"\n",
    "class Attention_NoRMask(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates the attention network, building the downwards and upwards paths of the UNet.\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pass along batch size to ensure dimensions are consistent\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Downsampling path of the U-Net\n",
    "        # \"We used a standard U-Net blueprint with five blocks each on the downsampling and upsampling paths.\"\n",
    "        # \"At the kth attention step, the attention network receives the concatenation of the input image x ...\n",
    "        #     and the current scope mask in log units, logsk, as input.\"\n",
    "        # \"Each block consists of the following: a 3x3 bias-free convolution with stride 1, ...\n",
    "        #     followed by instance normalisation with a learned bias term, followed by ...\n",
    "        #     a ReLU activation, and finally downsampled or upsampled by a factor of 2 using ...\n",
    "        #     nearest neighbour-resizing (no resizing occurs in the last block of each path).\"\n",
    "        # Block #1\n",
    "        self.down_nn_1 = nn.Sequential(\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=4,      # \"the concatenation of the input image x and the current scope mask\"\n",
    "                out_channels=8,     # ASSUMPTION\n",
    "                kernel_size=3,      # \"3x3 bias-free convolution\"\n",
    "                stride=1,           # \"stride 1\"\n",
    "                padding=1,          # ASSUMPTION\n",
    "                bias=False,         # \"bias-free\"\n",
    "            ),                      # 64*64 output because I=64, K=3, S=1, P=1, floor((64-3+2(1))/1)+1=64\n",
    "            nn.InstanceNorm2d(      # \"instance normalisation\"\n",
    "                num_features=8,     # FROM PREVIOUS LAYER\n",
    "                affine=True,        # \"with a learned bias term\"\n",
    "            ),                      # 64*64 output maintained\n",
    "            nn.ReLU(),              # \"ReLU activation\"\n",
    "        )\n",
    "        self.down_sample_1 = nn.Sequential(\n",
    "            nn.AvgPool2d(           # \"and finally downsampled or upsampled\"\n",
    "                kernel_size=2,      # \"by a factor of 2\"\n",
    "                stride=2,           # \"by a factor of 2\"\n",
    "                padding=0,          # ASSUMPTION\n",
    "                ceil_mode=True,     # ASSUMPTION\n",
    "            ),                      # 32*32 output because I=64, K=2, S=2, P=0, ceil((64-2+2(0))/2)+1=32\n",
    "        )\n",
    "        # Block #2\n",
    "        self.down_nn_2 = nn.Sequential(\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=8,      # FROM PREVIOUS LAYER\n",
    "                out_channels=16,    # ASSUMPTION\n",
    "                kernel_size=3,      # \"3x3 bias-free convolution\"\n",
    "                stride=1,           # \"stride 1\"\n",
    "                padding=1,          # ASSUMPTION\n",
    "                bias=False,         # \"bias-free\"\n",
    "            ),                      # 32*32 output because I=32, K=3, S=1, P=1, floor((32-3+2(1))/1)+1=32\n",
    "            nn.InstanceNorm2d(      # \"instance normalisation\"\n",
    "                num_features=16,    # FROM PREVIOUS LAYER\n",
    "                affine=True,        # \"with a learned bias term\"\n",
    "            ),                      # 32*32 output maintained\n",
    "            nn.ReLU(),              # \"ReLU activation\"\n",
    "        )\n",
    "        self.down_sample_2 = nn.Sequential(\n",
    "            nn.AvgPool2d(           # \"and finally downsampled or upsampled\"\n",
    "                kernel_size=2,      # \"by a factor of 2\"\n",
    "                stride=2,           # \"by a factor of 2\"\n",
    "                padding=0,          # ASSUMPTION\n",
    "                ceil_mode=True,     # ASSUMPTION\n",
    "            ),                      # 16*16 output because I=29, K=2, S=2, P=0, ceil((32-2+2(0))/2)+1=16\n",
    "        )\n",
    "        # Block #3\n",
    "        self.down_nn_3 = nn.Sequential(\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=16,     # FROM PREVIOUS LAYER\n",
    "                out_channels=32,    # ASSUMPTION\n",
    "                kernel_size=3,      # \"3x3 bias-free convolution\"\n",
    "                stride=1,           # \"stride 1\"\n",
    "                padding=1,          # ASSUMPTION\n",
    "                bias=False,         # \"bias-free\"\n",
    "            ),                      # 16*16 output because I=16, K=3, S=1, P=1, floor((16-3+2(1))/1)+1=16\n",
    "            nn.InstanceNorm2d(      # \"instance normalisation\"\n",
    "                num_features=32,    # FROM PREVIOUS LAYER\n",
    "                affine=True,        # \"with a learned bias term\"\n",
    "            ),                      # 16*16 output maintained\n",
    "            nn.ReLU(),              # \"ReLU activation\"\n",
    "        )\n",
    "        self.down_sample_3 = nn.Sequential(\n",
    "            nn.AvgPool2d(           # \"and finally downsampled or upsampled\"\n",
    "                kernel_size=2,      # \"by a factor of 2\"\n",
    "                stride=2,           # \"by a factor of 2\"\n",
    "                padding=0,          # ASSUMPTION\n",
    "                ceil_mode=True,     # ASSUMPTION\n",
    "            ),                      # 8*8 output because I=16, K=2, S=2, P=0, ceil((16-2+2(0))/2)+1=8\n",
    "        )\n",
    "        # Block #4\n",
    "        self.down_nn_4 = nn.Sequential(\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=32,     # FROM PREVIOUS LAYER\n",
    "                out_channels=64,    # ASSUMPTION\n",
    "                kernel_size=3,      # \"3x3 bias-free convolution\"\n",
    "                stride=1,           # \"stride 1\"\n",
    "                padding=1,          # ASSUMPTION\n",
    "                bias=False,         # \"bias-free\"\n",
    "            ),                      # 8*8 output because I=8, K=3, S=1, P=1, floor((8-3+2(1))/1)+1=8\n",
    "            nn.InstanceNorm2d(      # \"instance normalisation\"\n",
    "                num_features=64,    # FROM PREVIOUS LAYER\n",
    "                affine=True,        # \"with a learned bias term\"\n",
    "            ),                      # 8*8 output maintained\n",
    "            nn.ReLU(),              # \"ReLU activation\"\n",
    "        )\n",
    "        self.down_sample_4 = nn.Sequential(\n",
    "            nn.AvgPool2d(           # \"and finally downsampled or upsampled\"\n",
    "                kernel_size=2,      # \"by a factor of 2\"\n",
    "                stride=2,           # \"by a factor of 2\"\n",
    "                padding=0,          # ASSUMPTION\n",
    "                ceil_mode=True,     # ASSUMPTION\n",
    "            ),                      # 4*4 output because I=5, K=2, S=2, P=0, ceil((8-2+2(0))/2)+1=4\n",
    "        )\n",
    "        # Block #5\n",
    "        self.down_nn_5 = nn.Sequential(\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=64,     # FROM PREVIOUS LAYER\n",
    "                out_channels=128,   # ASSUMPTION\n",
    "                kernel_size=3,      # \"3x3 bias-free convolution\"\n",
    "                stride=1,           # \"stride 1\"\n",
    "                padding=1,          # ASSUMPTION\n",
    "                bias=False,         # \"bias-free\"\n",
    "            ),                      # 4*4 output because I=4, K=3, S=1, P=1, floor((4-3+2(1))/1)+1=4\n",
    "            nn.InstanceNorm2d(      # \"instance normalisation\"\n",
    "                num_features=128,   # FROM PREVIOUS LAYER\n",
    "                affine=True,        # \"with a learned bias term\"\n",
    "            ),                      # 4*4 output maintained\n",
    "            nn.ReLU(),              # \"ReLU activation\"\n",
    "            # \"no resizing occurs in the last block of each path\"\n",
    "        )\n",
    "\n",
    "        # Skip connections of the UNet\n",
    "        # \"Skip tensors are collected from each block in the downsampling path ...\n",
    "        #     after the ReLU activation function. These are concatenated with input ...\n",
    "        #     tensors along the upsampling blocks before the convolutional layer.\"\n",
    "        self.skip_nn = nn.Sequential(\n",
    "            # Implemented in forward() function\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        # Nonskip connections of the UNet\n",
    "        # # \"A 3-layer MLP serves as the non-skip connection between the downsampling and ...\n",
    "        #     upsampling paths with its final output dimensionally matching that of the ...\n",
    "        #     last skip tensor.\"\"\n",
    "        self.middle_nn = nn.Sequential(\n",
    "            # \"The intermediate hidden layers were sized (128, 128). The input to the MLP is ...\n",
    "            #     the last skip tensor collected from the downsampling path (after flattening). ...\n",
    "            #     A ReLU activation is applied after all three output layers. The final output is ...\n",
    "            #     then reshaped to match that of the last skip tensor, concatenated with it, ...\n",
    "            #     and finally fed into the upsampling path.\"\n",
    "            nn.Flatten(),\n",
    "            # Layer #1\n",
    "            nn.Linear(              # \"MLP\"\n",
    "                in_features=2048,   # FROM PREVIOUS LAYER\n",
    "                out_features=128,   # \"128\"\n",
    "            ),\n",
    "            nn.ReLU(),              # \"ReLU activation\"\n",
    "            # Layer #2\n",
    "            nn.Linear(              # \"MLP\"\n",
    "                in_features=128,    # FROM PREVIOUS LAYER\n",
    "                out_features=128,   # \"128\"\n",
    "            ),\n",
    "            nn.ReLU(),              # \"ReLU activation\"\n",
    "            # Layer #3\n",
    "            nn.Linear(              # \"MLP\"\n",
    "                in_features=128,    # FROM PREVIOUS LAYER\n",
    "                out_features=2048,  # \"reshaped to match that of the last skip tensor\"\n",
    "            ),\n",
    "            nn.ReLU(),              # \"ReLU activation\"\n",
    "        )\n",
    "\n",
    "        # \"We used a standard U-Net blueprint with five blocks each on the downsampling and upsampling paths.\"\n",
    "        # \"Each block consists of the following: a 3x3 bias-free convolution with stride 1, ...\n",
    "        #     followed by instance normalisation with a learned bias term, followed by ...\n",
    "        #     a ReLU activation, and finally downsampled or upsampled by a factor of 2 using ...\n",
    "        #     nearest neighbour-resizing (no resizing occurs in the last block of each path).\"\n",
    "        # Block #1\n",
    "        self.up_nn_1 = nn.Sequential(\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=256,    # FROM PREVIOUS LAYER\n",
    "                out_channels=64,    # ASSUMPTION\n",
    "                kernel_size=3,      # \"3x3 bias-free convolution\"\n",
    "                stride=1,           # \"stride 1\"\n",
    "                padding=1,          # ASSUMPTION\n",
    "                bias=False,         # \"bias-free\"\n",
    "            ),                      # 4*4 output because I=4, K=3, S=1, P=1, floor((4-3+2(1))/1)+1=4\n",
    "            nn.InstanceNorm2d(      # \"instance normalisation\"\n",
    "                num_features=64,    # FROM PREVIOUS LAYER\n",
    "                affine=True,        # \"with a learned bias term\"\n",
    "            ),                      # 4*4 output maintained\n",
    "            nn.ReLU(),              # \"ReLU activation\"\n",
    "            nn.Upsample(            # \"and finally downsampled or upsampled\"\n",
    "                scale_factor=2,     # \"by a factor of 2\"\n",
    "                mode=\"nearest\",     # \"using nearest neighbour-resizing\"\n",
    "            ),\n",
    "        )\n",
    "        # Block #2\n",
    "        self.up_nn_2 = nn.Sequential(\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=128,    # FROM PREVIOUS LAYER\n",
    "                out_channels=32,    # ASSUMPTION\n",
    "                kernel_size=3,      # \"3x3 bias-free convolution\"\n",
    "                stride=1,           # \"stride 1\"\n",
    "                padding=1,          # ASSUMPTION\n",
    "                bias=False,         # \"bias-free\"\n",
    "            ),                      # 8*8 output because I=8, K=3, S=1, P=1, floor((8-3+2(1))/1)+1=8\n",
    "            nn.InstanceNorm2d(      # \"instance normalisation\"\n",
    "                num_features=32,    # FROM PREVIOUS LAYER\n",
    "                affine=True,        # \"with a learned bias term\"\n",
    "            ),                      # 8*8 output maintained\n",
    "            nn.ReLU(),              # \"ReLU activation\"\n",
    "            nn.Upsample(            # \"and finally downsampled or upsampled\"\n",
    "                scale_factor=2,     # \"by a factor of 2\"\n",
    "                mode=\"nearest\",     # \"using nearest neighbour-resizing\"\n",
    "            ),\n",
    "        )\n",
    "        # Block #3\n",
    "        self.up_nn_3 = nn.Sequential(\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=64,     # FROM PREVIOUS LAYER\n",
    "                out_channels=16,    # ASSUMPTION\n",
    "                kernel_size=3,      # \"3x3 bias-free convolution\"\n",
    "                stride=1,           # \"stride 1\"\n",
    "                padding=1,          # ASSUMPTION\n",
    "                bias=False,         # \"bias-free\"\n",
    "            ),                      # 16*16 output because I=16, K=3, S=1, P=1, floor((16-3+2(1))/1)+1=16\n",
    "            nn.InstanceNorm2d(      # \"instance normalisation\"\n",
    "                num_features=16,    # FROM PREVIOUS LAYER\n",
    "                affine=True,        # \"with a learned bias term\"\n",
    "            ),                      # 16*16 output maintained\n",
    "            nn.ReLU(),              # \"ReLU activation\"\n",
    "            nn.Upsample(            # \"and finally downsampled or upsampled\"\n",
    "                scale_factor=2,     # \"by a factor of 2\"\n",
    "                mode=\"nearest\",     # \"using nearest neighbour-resizing\"\n",
    "            ),\n",
    "        )\n",
    "        # Block #4\n",
    "        self.up_nn_4 = nn.Sequential(\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=32,     # FROM PREVIOUS LAYER\n",
    "                out_channels=8,     # ASSUMPTION\n",
    "                kernel_size=3,      # \"3x3 bias-free convolution\"\n",
    "                stride=1,           # \"stride 1\"\n",
    "                padding=1,          # ASSUMPTION\n",
    "                bias=False,         # \"bias-free\"\n",
    "            ),                      # 32*32 output because I=32, K=3, S=1, P=1, floor((32-3+2(1))/1)+1=32\n",
    "            nn.InstanceNorm2d(      # \"instance normalisation\"\n",
    "                num_features=8,     # FROM PREVIOUS LAYER\n",
    "                affine=True,        # \"with a learned bias term\"\n",
    "            ),                      # 32*32 output maintained\n",
    "            nn.ReLU(),              # \"ReLU activation\"\n",
    "            nn.Upsample(            # \"and finally downsampled or upsampled\"\n",
    "                scale_factor=2,     # \"by a factor of 2\"\n",
    "                mode=\"nearest\",     # \"using nearest neighbour-resizing\"\n",
    "            ),\n",
    "        )\n",
    "        # Block #5\n",
    "        self.up_nn_5 = nn.Sequential(\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=16,     # FROM PREVIOUS LAYER\n",
    "                out_channels=4,     # ASSUMPTION\n",
    "                kernel_size=3,      # \"3x3 bias-free convolution\"\n",
    "                stride=1,           # \"stride 1\"\n",
    "                padding=1,          # ASSUMPTION\n",
    "                bias=False,         # \"bias-free\"\n",
    "            ),                      # 64*64 output because I=64, K=3, S=1, P=1, floor((64-3+2(1))/1)+1=64\n",
    "            nn.InstanceNorm2d(      # \"instance normalisation\"\n",
    "                num_features=4,     # FROM PREVIOUS LAYER\n",
    "                affine=True,        # \"with a learned bias term\"\n",
    "            ),                      # 64*64 output maintained\n",
    "            nn.ReLU(),              # \"ReLU activation\"\n",
    "            # \"no resizing occurs in the last block of each path\"\n",
    "        )\n",
    "\n",
    "        # \"Following the upsampling path, a final 1x1 convolution with stride 1 ...\n",
    "        #     and a single output channel transforms the U-Net output into the ...\n",
    "        #     logits for αk. Both log αk and log(1 − αk) are computed directly in ...\n",
    "        #     log units from the logits (using the log softmax operation). Each are ...\n",
    "        #     added to the current scope (also maintained in log units) log sk−1 to ...\n",
    "        #     compute the next (log) attention mask log mk and next (log) scope log sk, respectively.\"\"\n",
    "        self.final_nn = nn.Sequential(\n",
    "            nn.Conv2d(              # \"convolution\"\n",
    "                in_channels=4,      # FROM PREVIOUS LAYER\n",
    "                out_channels=1,     # \"a single output channel\"\n",
    "                kernel_size=1,      # \"1x1 convolution\"\n",
    "                stride=1,           # \"stride 1\"\n",
    "                padding=0,          # ASSUMPTION\n",
    "                bias=True,          # ASSUMPTION\n",
    "            ),                      # 64*64 output because I=64, K=1, S=1, P=0, floor((64-1+2(0))/1)+1=64\n",
    "        )\n",
    "        return\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs a full forward pass of the attention network, updating internal state.\n",
    "        Inputs: 64*64 RGB image (x), current logarithmic scope (log_sk)\n",
    "        Outputs: 64*64 logarithmic mask (log_mk), next logarithmic scope (log_skp1)\n",
    "    \"\"\"\n",
    "    def forward(self, x, log_sk):\n",
    "        # First, downsample the data down the \"U\" of the UNet\n",
    "        x1 = self.down_nn_1(torch.concat((x, log_sk), dim=1))\n",
    "        # assert (len(x1.shape) == 4) and (x1.shape[0] == self.batch_size) and (x1.shape[1] == 8) and (x1.shape[2] == 64) and (x1.shape[3] == 64)\n",
    "        x2 = self.down_nn_2(self.down_sample_1(x1))\n",
    "        # assert (len(x2.shape) == 4) and (x2.shape[0] == self.batch_size) and (x2.shape[1] == 16) and (x2.shape[2] == 32) and (x2.shape[3] == 32)\n",
    "        x3 = self.down_nn_3(self.down_sample_2(x2))\n",
    "        # assert (len(x3.shape) == 4) and (x3.shape[0] == self.batch_size) and (x3.shape[1] == 32) and (x3.shape[2] == 16) and (x3.shape[3] == 16)\n",
    "        x4 = self.down_nn_4(self.down_sample_3(x3))\n",
    "        # assert (len(x4.shape) == 4) and (x4.shape[0] == self.batch_size) and (x4.shape[1] == 64) and (x4.shape[2] == 8) and (x4.shape[3] == 8)\n",
    "        x5 = self.down_nn_5(self.down_sample_4(x4))\n",
    "        # assert (len(x5.shape) == 4) and (x5.shape[0] == self.batch_size) and (x5.shape[1] == 128) and (x5.shape[2] == 4) and (x5.shape[3] == 4)\n",
    "\n",
    "        # Second, compute the nonskip connection at the bottom of the \"U\" of the UNet\n",
    "        y0 = self.middle_nn(x5).reshape((self.batch_size, 128, 4, 4))\n",
    "        # assert (len(y0.shape) == 4) and (y0.shape[0] == self.batch_size) and (y0.shape[1] == 128) and (y0.shape[2] == 4) and (y0.shape[3] == 4)\n",
    "\n",
    "        # Third, upsample the data up the \"U\" of the UNet\n",
    "        # Successively call upsampling networks on processed data\n",
    "        # Apply skip connections by concatenating previous data to the current data\n",
    "        y1 = self.up_nn_1(torch.concat((y0, x5), dim=1))\n",
    "        # assert (len(y1.shape) == 4) and (y1.shape[0] == self.batch_size) and (y1.shape[1] == 64) and (y1.shape[2] == 8) and (y1.shape[3] == 8)\n",
    "        y2 = self.up_nn_2(torch.concat((y1, x4), dim=1))\n",
    "        # assert (len(y2.shape) == 4) and (y2.shape[0] == self.batch_size) and (y2.shape[1] == 32) and (y2.shape[2] == 16) and (y2.shape[3] == 16)\n",
    "        y3 = self.up_nn_3(torch.concat((y2, x3), dim=1))\n",
    "        # assert (len(y3.shape) == 4) and (y3.shape[0] == self.batch_size) and (y3.shape[1] == 16) and (y3.shape[2] == 32) and (y3.shape[3] == 32)\n",
    "        y4 = self.up_nn_4(torch.concat((y3, x2), dim=1))\n",
    "        # assert (len(y4.shape) == 4) and (y4.shape[0] == self.batch_size) and (y4.shape[1] == 8) and (y4.shape[2] == 64) and (y4.shape[3] == 64)\n",
    "        y5 = self.up_nn_5(torch.concat((y4, x1), dim=1))\n",
    "        # assert (len(y5.shape) == 4) and (y5.shape[0] == self.batch_size) and (y5.shape[1] == 4) and (y5.shape[2] == 64) and (y5.shape[3] == 64)\n",
    "\n",
    "        # Fourth, compute the output processing at the end of the UNet\n",
    "        y = self.final_nn(y5)\n",
    "        # assert (len(y.shape) == 4) and (y.shape[0] == self.batch_size) and (y.shape[1] == 1) and (y.shape[2] == 64) and (y.shape[3] == 64)\n",
    "        log_alpha = nn.LogSigmoid()(y)\n",
    "        # assert (len(log_alpha.shape) == 4) and (log_alpha.shape[0] == self.batch_size) and (log_alpha.shape[1] == 1) and (log_alpha.shape[2] == 64) and (log_alpha.shape[3] == 64)\n",
    "\n",
    "        # Fifth, translate the network output to the desired format\n",
    "        # \"The attention mask for step k is given by mk = sk−1αψ(x;sk−1)\"\n",
    "        log_mk = log_sk + log_alpha\n",
    "        # assert (len(log_mk.shape) == 4) and (log_mk.shape[0] == self.batch_size) and (log_mk.shape[1] == 1) and (log_mk.shape[2] == 64) and (log_mk.shape[3] == 64)\n",
    "        # \"The scope for the next step is given by sk+1 = sk(1−αψ(x;sk))\"\n",
    "        log_skp1 = log_sk + log_alpha - y\n",
    "        # assert (len(log_skp1.shape) == 4) and (log_skp1.shape[0] == self.batch_size) and (log_skp1.shape[1] == 1) and (log_skp1.shape[2] == 64) and (log_skp1.shape[3] == 64)\n",
    "\n",
    "        # Output of the attention network is the mask at the current time step\n",
    "        return log_mk, log_skp1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination: MONet\n",
    "\n",
    "#### Changes\n",
    "\n",
    "Only adjusting the interfaces of each step in the forward pass and removing the features supporting the third term of the loss function was required to make MONet work with this updated VAE. No other changes were required for the overall MONet model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Overall MONet model.\n",
    "\"\"\"\n",
    "class MONet_NoRMask(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates MONet, building the VAE and attention network.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_slots, batch_size, learning_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        # Model general hyperparameters\n",
    "        # \"We used RMSProp for optimisation with a learning rate of 0.0001, and a batch size of 64.\"\n",
    "        self.batch_size = batch_size             # \"a batch size of 64\"\n",
    "        self.learning_rate = learning_rate       # \"a learning rate of 0.0001\"\n",
    "\n",
    "        # Model construction hyperparameters\n",
    "        # \"We trained MONet with K=7 slots.\"\n",
    "        self.K = num_slots    # \"K=7 slots\"\n",
    "        # \"The loss weights were β = 0.5, γ = 0.5.\"\n",
    "        self.alpha = 1\n",
    "        self.beta = 0.5       # \"β = 0.5\"\n",
    "        # \"For the MONet experiments, the first \"background\" component scale was ...\n",
    "        #     fixed at σbg = 0.09, and for the K − 1 remaining \"foreground\" components, ...\n",
    "        #     the scale was fixed at σfg = 0.11.\n",
    "        self.sigma_bg = 0.09    # \"σbg = 0.09\"\n",
    "        self.sigma_fg = 0.11    # \"σfg = 0.11\"\n",
    "\n",
    "        # Model loss\n",
    "        # First term represents the VAE image reconstruction loss (drives the decoder to properly reconstruct masked region)\n",
    "        self.loss_1 = None\n",
    "        # Second term represents the regularization of the VAE (drives the encoder to generate a normal distribution) weighted by beta\n",
    "        self.loss_2 = None\n",
    "        # Overall loss combines these two terms (CHANGE)\n",
    "        self.loss = None\n",
    "\n",
    "        # VAE of the model\n",
    "        # \"The component VAE is a neural network, with an encoder parameterised by φ and a decoder parameterised by θ.\"\n",
    "        self.vae = VAE_NoRMask(self.batch_size)\n",
    "\n",
    "        # Attention network of the model\n",
    "        # \"The mask distribution is learned by the attention module, a neural network conditioned on x and parameterised by ψ.\"\n",
    "        self.attention = Attention_NoRMask(self.batch_size)\n",
    "\n",
    "        # Model optimizer\n",
    "        # \"We used RMSProp for optimisation with a learning rate of 0.0001, and a batch size of 64.\"\n",
    "        self.optimizer = torch.optim.RMSprop(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "        )\n",
    "        return\n",
    "\n",
    "    \"\"\"\n",
    "    Performs a full forward pass of MONet, utilizing multiple passes of the VAE and attention network.\n",
    "        Inputs: 64*64 RGB image (x)\n",
    "        Outputs: 16-dimensional Gaussian latent posteriors (mus, log_sigs), 64*64 logarithmic masks (log_masks), ...\n",
    "            64*64 RGB reconstructed image components means (x_hat_means)\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        # The parameters of the latent distributions generated by the VAE\n",
    "        mus = torch.zeros((self.K, self.batch_size, 16), device=device)\n",
    "        log_sigs = torch.zeros((self.K, self.batch_size, 16), device=device)\n",
    "        # The masks recurrently generated by the attention network\n",
    "        log_masks = torch.zeros((self.K, self.batch_size, 1, 64, 64), device=device)\n",
    "        # The current state of the model\n",
    "        # Initialize recurrent state variable \"with the first scope s0 = 1\"\n",
    "        log_states = torch.zeros((self.K, self.batch_size, 1, 64, 64), device=device)\n",
    "        log_states[0] = torch.log(torch.ones((self.batch_size, 1, 64, 64), device=device))\n",
    "        # The region of the image corresponding to the mask as reconstructed by the VAE\n",
    "        recon_comp_means = torch.zeros((self.K, self.batch_size, 3, 64, 64), device=device)\n",
    "\n",
    "        # Perform normal steps 1, ..., K-1\n",
    "        for k in range(self.K - 1):\n",
    "            # Mask k is simply the output of the attention network based on the image (and its current internal state)\n",
    "            log_masks[k], log_states[k+1] = self.attention(x, log_states[k])\n",
    "            # Component and mask reconstruction k is the output of the VAE based on the image and desired attention mask\n",
    "            mus[k], log_sigs[k], recon_comp_means[k] = self.vae(x, log_masks[k])\n",
    "        # Perform final step K, which is different\n",
    "        # Mask K is the remaining scope to be explained, extracted directly from the attention network\n",
    "        log_masks[self.K-1] = log_states[self.K-1]\n",
    "        # Component and mask reconstruction K is still the normal output of the VAE\n",
    "        mus[self.K-1], log_sigs[self.K-1], recon_comp_means[self.K-1] = self.vae(x, log_masks[self.K-1])\n",
    "\n",
    "        # assert (len(mus.shape) == 3) and (mus.shape[0] == self.K) and (mus.shape[1] == self.batch_size) and (mus.shape[2] == 16)\n",
    "        # assert (len(log_sigs.shape) == 3) and (log_sigs.shape[0] == self.K) and (log_sigs.shape[1] == self.batch_size) and (log_sigs.shape[2] == 16)\n",
    "        # assert (len(log_masks.shape) == 5) and (log_masks.shape[0] == self.K) and (log_masks.shape[1] == self.batch_size) and (log_masks.shape[2] == 1) and (log_masks.shape[3] == 64) and (log_masks.shape[4] == 64)\n",
    "        # assert (len(log_states.shape) == 5) and (log_states.shape[0] == self.K) and (log_states.shape[1] == self.batch_size) and (log_states.shape[2] == 1) and (log_states.shape[3] == 64) and (log_states.shape[4] == 64)\n",
    "        # assert (len(recon_comp_means.shape) == 5) and (recon_comp_means.shape[0] == self.K) and (recon_comp_means.shape[1] == self.batch_size) and (recon_comp_means.shape[2] == 3) and (recon_comp_means.shape[3] == 64) and (recon_comp_means.shape[4] == 64)\n",
    "        masks_sum = torch.sum(log_masks.exp()) / self.batch_size / 64 / 64\n",
    "        if (masks_sum < 0.99) or (masks_sum > 1.01):\n",
    "            print(f\"WARNING: Mask distributions do not sum to 1. Computed value: {masks_sum}\")\n",
    "\n",
    "        # Output of MONet is the parameters of the probability distribution, mask, and reconstructed image and mask at the current time step\n",
    "        return mus, log_sigs, log_masks, recon_comp_means\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training: Loss Function\n",
    "\n",
    "#### Changes\n",
    "\n",
    "The first two terms of the loss function are unaffected by these changes; however, the third term directly refers the reconstructed masks. Because of this direct dependency, this loss term was completely removed (no other option exists). This eliminates the regularization-like objective that ensures the attention network generates simple masks; indeed, the masks generated by NoRMask MONet tend to be more complex without this objective, but this adjustment has surprisingly little impact in practice (especially on simple datasets). No other changes were required for the loss function and training objective; the first two terms of the loss function were used to train the model alone, and the model was still trained end-to-end jointly on the combination of these two objectives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train MONet.\n",
    "    Inputs: Data to train on (dataloaders), number of iterations to train for (epochs), location where model should be stored (name)\n",
    "    Outputs: Saved model weights\n",
    "\"\"\"\n",
    "def learn(self, dataloaders, epochs, name):\n",
    "    # Create directory to save model in, if it does not yet exist\n",
    "    if not os.path.exists(f\"models/{name}/\"):\n",
    "        os.makedirs(f\"models/{name}/\")\n",
    "\n",
    "    # Set up optimized training\n",
    "    scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "    losses = []\n",
    "    # Iterate over each training epoch\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        start = time()\n",
    "        losses.append([])\n",
    "        # Iterate through each dataset to train on\n",
    "        for dataloader in dataloaders:\n",
    "            # Iterate through each batch of the dataset\n",
    "            for i, x in enumerate(dataloader):\n",
    "                # Extract image from training data (unsupervised, so this is all that is needed)\n",
    "                image = (x[\"image\"] / 255).to(device)\n",
    "                # Only deal with complete batches for simplicity's sake\n",
    "                if image.shape[0] != self.batch_size:\n",
    "                    break\n",
    "\n",
    "                # Perform forward pass, computing model outputs in optimized manner\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                with torch.autocast(device):\n",
    "                    # Perform complete forward pass of the complete model\n",
    "                    mus, log_sigs, log_masks, recon_comp_means = self(image)\n",
    "\n",
    "                    # Compute reconstruction loss term\n",
    "                    # Image reconstruction standard deviations are uniform across images but different for foreground and background slots\n",
    "                    recon_comp_sigmas = torch.Tensor([self.sigma_bg if k == 0 else self.sigma_fg for k in range(self.K)])\n",
    "                    # Sum up contribution of each slot to the reconstruction loss\n",
    "                    reconstruction_loss = 0\n",
    "                    for j in range(0, self.K):\n",
    "                        # Use derived formula to compute weighted difference\n",
    "                        # \"the VAE’s decoder likelihood term in the loss pθ(x|zk) is weighted according ...\n",
    "                        #     to the mask, such that it is unconstrained outside of the masked regions.\"\n",
    "                        reconstruction_loss = reconstruction_loss + torch.exp(log_masks[j] - torch.log(recon_comp_sigmas[j]) - 0.5 * (image - recon_comp_means[j]).pow(2) / recon_comp_sigmas[j].pow(2))\n",
    "                    # Negative log operation required to convert to proper loss function, summed across all pixels\n",
    "                    reconstruction_loss = torch.sum(-torch.log(reconstruction_loss))\n",
    "                    # Weight loss term by alpha hyperparameter\n",
    "                    self.loss_1 = self.alpha * reconstruction_loss / self.batch_size\n",
    "\n",
    "                    # Compute VAE KL divergence loss term\n",
    "                    # Each encoded representation is independent, so the KL divergence is additive\n",
    "                    kld_loss = 0\n",
    "                    for j in range(0, self.K):\n",
    "                        # Use closed-form expression from class to compute KL divergence\n",
    "                        kld_loss = kld_loss + torch.sum(torch.exp(log_sigs[j]).pow(2) + mus[j].pow(2) - 2 * log_sigs[j] - 1) / 2\n",
    "                    # Weight loss term by beta hyperparameter\n",
    "                    self.loss_2 = self.beta * kld_loss / self.batch_size\n",
    "\n",
    "                    # Compute overall loss\n",
    "                    self.loss = self.loss_1 + self.loss_2\n",
    "\n",
    "                # Perform backward pass, gradient descent update on all parameters\n",
    "                scaler.scale(self.loss).backward()\n",
    "                scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=0.05)\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                # Track training details\n",
    "                losses[epoch].append((self.loss.detach().item(), self.loss_1.detach().item(), self.loss_2.detach().item()))\n",
    "\n",
    "        # Save model weights for subsequent inference\n",
    "        torch.save(self, f\"models/{name}/model_epoch_{epoch}.pt\")\n",
    "        torch.save(self, f\"models/{name}/model_final.pt\")\n",
    "        # Print training details\n",
    "        print(f\"Epoch {epoch} completed in {time()-start} seconds\")\n",
    "        print(f\"\\tAverage Loss: ({sum([value[0] / len(dataloaders) / len(dataloaders[0]) for value in losses[-1]])}, {sum([value[1] / len(dataloaders) / len(dataloaders[0]) for value in losses[-1]])}, {sum([value[2] / len(dataloaders) / len(dataloaders[0]) for value in losses[-1]])})\")\n",
    "\n",
    "    # Save final model\n",
    "    torch.save(self, f\"models/{name}/model_final.pt\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We hope this annotated write-up clarifies the changes made to produce our NoRMask extension of the MONet model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
